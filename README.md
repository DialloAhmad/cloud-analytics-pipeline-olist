## üìã Pr√©sentation du Projet

Ce projet d√©montre la mise en place d'une infrastructure **Data Engineering moderne et automatis√©e**.
L'objectif √©tait de simuler un environnement de production r√©el o√π les donn√©es transactionnelles arrivent en continu, sont ing√©r√©es automatiquement, transform√©es via des mod√®les analytiques, et visualis√©es en quasi temps r√©el.

Le projet s‚Äôappuie sur le dataset public **Olist E-Commerce**, repr√©sentant une marketplace br√©silienne reliant vendeurs ind√©pendants et clients finaux.

Au-del√† de l‚Äôaspect technique, ce projet met en lumi√®re des **enjeux business et soci√©taux concrets**, notamment :

* la fiabilit√© des d√©lais de livraison, essentielle √† la satisfaction client,
* la performance des vendeurs selon les r√©gions,
* l‚Äôimpact des co√ªts logistiques (freight) sur le chiffre d‚Äôaffaires,
* les disparit√©s g√©ographiques d‚Äôacc√®s au e-commerce.

Pour reproduire des conditions proches d'un environnement de production, les donn√©es ont √©t√© volontairement ing√©r√©es de mani√®re progressive (batchs), orchestr√©es par Apache Airflow, afin d‚Äô√©valuer les probl√©matiques d‚Äôautomatisation, de co√ªts et de performance propres aux plateformes data modernes.

## üèóÔ∏è Architecture du Pipeline

![Architecture Diagram](images/Dataset_Olist_pipeline.png)

### Le flux de donn√©es √©tape par √©tape :

1.  **Data Generation :** Un script Python d√©coupe les fichiers volumineux (`orders`, `order_items`) en 10 lots (chunks) pour simuler une arriv√©e de donn√©es quotidienne/horaire.
2.  **Orchestration (Airflow) :** Un DAG Airflow (g√©r√© via **Astro CLI** et **Cosmos**) upload un chunk vers AWS S3.
3.  **Continuous Ingestion (Snowpipe) :** Configuration d'un syst√®me **Event-Driven** (AWS S3 Event Notification -> SQS -> Snowpipe). D√®s qu'un fichier atterrit sur S3, Snowflake l'ing√®re automatiquement dans la couche `RAW`.
4.  **Transformation (dbt) :** Airflow d√©clenche les mod√®les dbt (via Cosmos) *uniquement* apr√®s avoir confirm√© l'ingestion des donn√©es par Snowpipe, transformant les donn√©es brutes en un mod√®le en √©toile (Kimball).
5.  **Visualization :** Power BI est connect√© en **Direct Query** √† Snowflake pour des tableaux de bord √† jour instantan√©ment.

## üõ†Ô∏è Focus Technique & D√©fis relev√©s

### 1. Ingestion "Event-Driven" avec Snowpipe
Au lieu d'un chargement manuel (`COPY INTO`), j'ai mis en place une automatisation via **AWS SNS/SQS**.
*   **Challenge :** Synchroniser l'orchestrateur (Airflow) avec un processus asynchrone (Snowpipe).
*   **Solution :** Le DAG Airflow upload le fichier, puis attend que la pipe soit "flushed" avant de lancer la suite.

![Capture Snowflake Pipe](images/snowpipe_loading_history.png)

### 2. Orchestration avanc√©e avec Airflow & Cosmos
J'ai utilis√© la biblioth√®que **Cosmos** pour int√©grer dbt comme citoyen de premi√®re classe dans Airflow. Cela permet de visualiser chaque mod√®le dbt comme une t√¢che distincte dans le DAG.

*   **Logique du DAG :** `Upload to S3` >> `Wait for Ingestion` >> `dbt Run (Staging -> Marts)` >> `Data Quality Tests`.

![Capture Airflow DAG](images/airflow_batches.png)

![Capture Airflow DAG](images/airflow_dags_graph.png)

### 3. Continuous Ingestion (Snowpipe & AWS)
Mise en place d'une architecture **Event-Driven** (pilot√©e par √©v√©nements) pour assurer un chargement des donn√©es en quasi temps r√©el, sans intervention manuelle.

*   **S3 Landing Zone (Data Lake) :** Configuration des buckets S3 pour recevoir les fichiers CSV d√©coup√©s (chunks). Organisation stricte des dossiers pour s√©parer les flux.
    *   ![Structure du Bucket S3 - Dossiers](images/s3_olist.png)
    *   ![Fichiers CSV Split√©s dans S3](images/s3_olist_orders.png)

*   **Event Architecture (SNS/SQS) :** Configuration des **S3 Event Notifications** pour d√©clencher un message automatique via SNS/SQS √† chaque nouvel upload. Cela permet de d√©coupler le stockage de l'ingestion.
    *   ![Configuration AWS SNS/SQS](images/sqs_suscribe.png)

*   **Automated Loading (Snowpipe) :** C√¥t√© Snowflake, un **Pipe** configur√© en `AUTO_INGEST=TRUE` √©coute la file d'attente SQS. D√®s qu'un message arrive, le fichier correspondant est charg√© instantan√©ment dans les tables `RAW`.

### 4. Mod√©lisation Dimensionnelle (dbt)
Transformation des donn√©es brutes vers un **Star Schema** (Mod√®le de Kimball) optimis√© pour l'analyse.
*   **Staging :** Nettoyage, typage et d√©duplication.
*   **Marts :** Cr√©ation de `FACT_SALES` et des dimensions (`DIM_PRODUCTS`, `DIM_CUSTOMERS`, etc.).
*   **Qualit√© :** Tests dbt (`unique`, `not_null`, `relationships`) int√©gr√©s au pipeline pour bloquer les donn√©es corrompues.

![Capture dbt Lineage](images/dbt_lineage_graph.png)

### 4. Advanced Analytics & Reporting (SQL)
Validation du mod√®le en √©toile par des requ√™tes analytiques complexes directement dans Snowflake.
L'exemple ci-dessous montre une analyse des "Top Cat√©gories Mensuelles" utilisant :
*   **CTEs (Common Table Expressions)** pour la lisibilit√©.
*   **Window Functions** (`RANK() OVER PARTITION`) pour le classement.
*   **Joins** entre la Fact Table et les Dimensions.

![Advanced SQL Query on Data Marts](images/analytique_query.png)

![Advanced SQL Query on Data Marts](images/analytique_query_result.png)


## üìä Business Intelligence (Power BI)

Le dashboard final permet de suivre les KPIs logistiques et financiers d'Olist. Gr√¢ce au **Direct Query**, toute nouvelle donn√©e trait√©e par le pipeline est imm√©diatement visible sans rafra√Æchissement manuel du dataset.

**KPIs Cl√©s :**
*   Revenue & Croissance monthly.
*   On-Time Delivery Rate (Performance Logistique).
*   Analyse g√©ographique des ventes.

![Dashboard Power BI](images/olist_dashboard.png)

## üèÖ Certifications & Badges Snowflake

Ce projet met en application les comp√©tences acquises lors de mes formations Snowflake.

*   ‚ùÑÔ∏è [**Data Engineering Workshop**](https://achieve.snowflake.com/4a69085a-2363-4222-8c7f-98b368b9704e#acc.K6umWHng)
*   ‚ùÑÔ∏è [**Data Lake Workshop**](https://achieve.snowflake.com/95cf50e3-f66c-4f7f-bf7b-5e212bac2214#acc.hjQKqy84)
*   ‚ùÑÔ∏è [**Data Warehousing Workshop**](https://achieve.snowflake.com/eceb04a5-7082-4eab-a009-2ad245dd3fe5#acc.AJHzaXGZ)
*   ‚ùÑÔ∏è [**Data Application Builders Workshop**](https://achieve.snowflake.com/89a954c3-936c-47d1-9f20-4baf0f16747b#acc.5yEpF4Pw)
*   ‚ùÑÔ∏è [**Collaboration, Marketplace & Cost Estimation**](https://achieve.snowflake.com/91c3f0d0-2376-4ef2-87af-d68dc42ec201#acc.ZOzQVcGJ)

---

## üë§ Auteur

**Thierno Amadou DIALLO**
*Cloud Data Engineer | Snowflake | AWS | dbt*

[LinkedIn](https://www.linkedin.com/in/thierno-amadou-diallo-84b4481b7/)
